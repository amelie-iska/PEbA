{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5EncoderModel: ['decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.final_layer_norm.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'lm_head.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import re\n",
    "\n",
    "# Load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL LOCAL ALIGNMENT TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize protein sequences (taken from two PH domain proteins)\n",
    "seq1 = 'AGFISVISKKQGEYLEDEWY'\n",
    "seq2 = 'QVLDKFGS'\n",
    "#seq1 = 'MRSSASRLSSFSSRDSLWNRMPDQISVSEFIAETTEDYNSPTTSSFTTRLHNCRNTVTLLEEALDQDRTALQKVKKSVKAIYNSGQDHVQNEENYAQVLDKFGSNFLSRDNPDLGTAFVKFSTLTKELSTLLKNLLQGLSHNVIFTLDSLLKGDLKGVKGDLKKPFDKAWKDYETKFTKIEKEKREHAKQHGMIRTEITGAEIAEEMEKERRLFQLQMCEYLIKVNEIKTKKGVDLLQNLIKYYHAQCNFFQDGLKTADKLKQYIEKLAADLYNIKQTQDEEKKQLTALRDLIKSSLQLDQKEDSQSRQGGYSMHQLQGN'\n",
    "#seq2 = 'NTVTLLEEALDQDRTALQKVKKSVKAIYNSGQDHVQNEENYAQVLDKFGSNFLSRDNPDLGTAF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_seqs(seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two protein sequences and returns a two lists of vectors, one list for\n",
    "    each protein where each vector represents one amino acid. Vectorization performed by the Rostlab\n",
    "    ProtT5-XL_UniRef50 model.\n",
    "\n",
    "    :param seq1: first protein sequence\n",
    "    :param seq2: second protein sequence\n",
    "    return: lists of vectorized amino acids\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Tokenize, encode, and load sequences\n",
    "    sequences = [' '.join([*seq1]), ' '.join([*seq2])]  # Add spaces between each amino acid\n",
    "    sequences = [re.sub(r\"[UZOB]\", \"X\", seq) for seq in sequences]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    # Extract sequence features\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "\n",
    "    # Remove padding and special tokens\n",
    "    features = [] \n",
    "    for seq_num in range(len(embedding)):\n",
    "        seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "        seq_emd = embedding[seq_num][:seq_len-1]\n",
    "        features.append(seq_emd)\n",
    "\n",
    "    # Return lists of vectors\n",
    "    seq1 = features[0]\n",
    "    seq2 = features[1]\n",
    "    return seq1, seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SW_align(seq1, seq2, vecs1, vecs2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two sequences, creates a matrix corresponding to their lengths, and  \n",
    "    calculates the score of the alignments for each index. A second matrix is scored so that the\n",
    "    best alignment can be tracebacked.\n",
    "\n",
    "    :param seq1: first sequence\n",
    "    :param seq2: second sequence\n",
    "    :param vecs1: list of vectorized amino acids for first sequence\n",
    "    :param vecs2: list of vectorized amino acids for second sequence\n",
    "    return: scoring and traceback matrices of optimal scores for the SW-alignment of sequences\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # NCBI default gap costs\n",
    "    gap_open = -11\n",
    "    gap_ext = -1\n",
    "    gap = False\n",
    "\n",
    "    # Protein alphabet\n",
    "    chars = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    # Initialize scoring and traceback matrix based on sequence lengths\n",
    "    row_length = len(seq1)+1\n",
    "    col_length = len(seq2)+1\n",
    "    score_m = np.full((row_length, col_length), 0)\n",
    "    trace_m = np.full((row_length, col_length), 0)\n",
    "\n",
    "    # Score matrix by moving through each index\n",
    "    for i in range(len(seq1)):\n",
    "        seq1_char = seq1[i]  # Character in 1st sequence\n",
    "        seq1_vec = vecs1[i]  # Corresponding amino acid vector\n",
    "        for j in range(len(seq2)):\n",
    "            seq2_char = seq2[j]\n",
    "            \n",
    "            # Preceding scoring matrix values\n",
    "            diagonal = score_m[i][j]\n",
    "            horizontal = score_m[i+1][j]\n",
    "            vertical = score_m[i][j+1]\n",
    "\n",
    "            # Score residues based off cosine similarity between vectors\n",
    "            print(seq1_vec, seq2_vec)\n",
    "            seq2_vec = vecs2[j]  # Corresponding amino acid vector\n",
    "            cos_sim = np.dot(seq1_vec,seq2_vec)/(np.linalg.norm(seq1_vec)*np.linalg.norm(seq2_vec))\n",
    "\n",
    "            '''TRYING OUT DIFFERENT COSINE SIMILARITY VALUE MODIFIERS'''\n",
    "            cos_sim = (cos_sim*10)\n",
    "\n",
    "            # Add to matrix values via scoring method\n",
    "            diagonal += cos_sim\n",
    "            if gap is False:  # Apply gap_open penalty if there is no gap\n",
    "                horizontal += gap_open\n",
    "                vertical += gap_open\n",
    "            if gap is True:  # Apply gap_extension penalty if there is a gap\n",
    "                horizontal += gap_ext\n",
    "                vertical += gap_ext\n",
    "\n",
    "            # Update gap status\n",
    "            score = max(diagonal, horizontal, vertical)\n",
    "            if score == horizontal: gap = True\n",
    "            if score == vertical: gap = True\n",
    "            if score == diagonal: gap = False\n",
    "\n",
    "            # Assign value to traceback matrix\n",
    "            if score == diagonal:\n",
    "                trace_m[i+1][j+1] = 0\n",
    "            if score == horizontal:\n",
    "                trace_m[i+1][j+1] = -1\n",
    "            if score == vertical:\n",
    "                trace_m[i+1][j+1] = -1\n",
    "\n",
    "            # Assign max value to scoring matrix\n",
    "            score_m[i+1][j+1] = max(score, 0)\n",
    "\n",
    "    return score_m, trace_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call vec_seqs() to get list of vectorized amino acids\n",
    "seq1_vecs, seq2_vecs = vec_seqs(seq1, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09522017, -0.14406678, -0.08793196, ...,  0.11808879,\n",
       "        -0.24785922,  0.00921392],\n",
       "       [ 0.21217601,  0.01319041, -0.05396945, ...,  0.43071347,\n",
       "        -0.18787555,  0.02300165],\n",
       "       [ 0.30360776, -0.11321972, -0.18153799, ...,  0.02647195,\n",
       "         0.12702596, -0.02317703],\n",
       "       [ 0.1496497 , -0.08891518, -0.12834446, ...,  0.02811868,\n",
       "         0.05552136,  0.17308137],\n",
       "       [ 0.00992799, -0.01431641,  0.22271894, ...,  0.1056217 ,\n",
       "        -0.01820021, -0.22538273]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq1_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call SW_align()\n",
    "score_m, trace_m = SW_align(seq1, seq2, seq1_vecs, seq2_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceback(score_m, trace_m, seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts a scoring and a traceback matrix and two sequences and returns the highest \n",
    "    scoring local alignment between the two sequences\n",
    "\n",
    "    :param score_m: scoring matrix\n",
    "    :param trace_m: traceback matrix\n",
    "    :param seq1: first sequence\n",
    "    :param seq2: second sequence\n",
    "    return: highest scoring local alignment of the two sequences\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Find index of highest score in scoring matrix, start traceback at this matrix\n",
    "    high_score_ind = np.unravel_index(np.argmax(score_m, axis=None), score_m.shape)\n",
    "\n",
    "    # Reverse strings and convert to lists so gaps can be inserted\n",
    "    rev_seq1 = list(seq1[::-1])\n",
    "    rev_seq2 = list(seq2[::-1])\n",
    "\n",
    "    # Move through matrix starting at bottom right\n",
    "    index = [high_score_ind[0], high_score_ind[1]]\n",
    "    count = 0\n",
    "    while (index[0] and index[1]) != 0:\n",
    "        val = trace_m[index[0], index[1]]\n",
    "\n",
    "        if val == 1:  # If cell is equal to 1, insert a gap into the second sequence\n",
    "            index[0] = index[0] - 1\n",
    "            rev_seq2.insert(count, '_')\n",
    "        if val == -1:  # If cell is equal to -1, insert a gap into the first sequence\n",
    "            index[1] = index[1] - 1\n",
    "            rev_seq1.insert(count, '_')\n",
    "        if val == 0:  # If cell is equal to 0, there is no gap\n",
    "            index[0] = index[0] - 1\n",
    "            index[1] = index[1] - 1\n",
    "        count += 1\n",
    "\n",
    "    # Join lists and reverse strings again\n",
    "    seq1 = ''.join(rev_seq1)\n",
    "    seq2 = ''.join(rev_seq2)\n",
    "\n",
    "    # Need to store alignment data somehow, but until then we are printing spaces\n",
    "    space = ' '\n",
    "    print(index[1]*space+seq1[::-1])\n",
    "    print(index[0]*space+seq2[::-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGFISVISKKQGEYLEDEWY\n",
      "            QVLDKFGS\n"
     ]
    }
   ],
   "source": [
    "traceback(score_m, trace_m, seq1, seq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING ON GLOBAL ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def parse_fasta(filename):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts a fasta file name and returns the sequence.\n",
    "\n",
    "    :param filename: name of file\n",
    "    return: sequence\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Parse fasta file\n",
    "    seq = ''\n",
    "    with open(filename, 'r', encoding='utf8') as file:\n",
    "        for seq in SeqIO.parse(file, 'fasta'):\n",
    "            seq = str(seq.seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_seq(seq, tokenizer, encoder):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts a protein sequence and returns a list of vectors, each vector representing\n",
    "    a single amino acid.\n",
    "\n",
    "    :param seq: protein sequence\n",
    "    :param: tokenizer: tokenizer model\n",
    "    :param encoder: encoder model\n",
    "    return: np array of vectors\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Add space after each amino acid so each residue is vectorized\n",
    "    print(seq)\n",
    "    seq = [' '.join([*seq])]\n",
    "    print(seq)\n",
    "\n",
    "    # Tokenize, encode, and load sequence\n",
    "    ids = tokenizer.batch_encode_plus(seq, add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids['input_ids'])\n",
    "    attention_mask = torch.tensor(ids['attention_mask'])\n",
    "\n",
    "    # Extract sequence features\n",
    "    with torch.no_grad():\n",
    "        embedding = encoder(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "\n",
    "    # Remove padding and special tokens\n",
    "    features = [] \n",
    "    for seq_num in range(len(embedding)):\n",
    "        seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "        seq_emd = embedding[seq_num][:seq_len-1]\n",
    "        features.append(seq_emd)\n",
    "    return features[0]  # Returns as a np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def global_align(seq1, seq2, vecs1, vecs2, gopen, gext):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two sequences, creates a matrix corresponding to their lengths, and\n",
    "    calculates the score of the alignments for each index. A second matrix is scored so that the\n",
    "    best alignment can be tracebacked.\n",
    "\n",
    "    :param seq1: first sequence\n",
    "    :param seq2: second sequence\n",
    "    :param vecs1: list of vectors for first sequence\n",
    "    :param vecs2: list of vectors for second sequence\n",
    "    :param gopen: gap penalty for opening a new gap\n",
    "    :param gext: gap penalty for extending a gap\n",
    "    return: traceback matrix\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Initialize scoring and traceback matrix based on sequence lengths\n",
    "    row_length = len(seq1)+1\n",
    "    col_length = len(seq2)+1\n",
    "    score_m = np.full((row_length, col_length), 0)\n",
    "    trace_m = np.full((row_length, col_length), 0)\n",
    "\n",
    "    # Initialize first row and column with gap values for scoring matrix\n",
    "    for i in range(1, len(score_m[0])):\n",
    "        score_m[0][i] = gopen+gext*i+1  # +1 to offset i starting at 1\n",
    "    for i in range(1, len(score_m.T[0])):\n",
    "        score_m.T[0][i] = gopen+gext*i+1\n",
    "\n",
    "    # Score matrix by moving through each index\n",
    "    gap = False\n",
    "    for i, char in enumerate(seq1):\n",
    "        seq1_vec = vecs1[i]  # Corresponding amino acid vector\n",
    "        for j, char in enumerate(seq2):\n",
    "\n",
    "            # Preceding scoring matrix values\n",
    "            diagonal = score_m[i][j]\n",
    "            horizontal = score_m[i+1][j]\n",
    "            vertical = score_m[i][j+1]\n",
    "\n",
    "            # Score residues based off cosine similarity between vectors\n",
    "            seq2_vec = vecs2[j]  # Corresponding amino acid vector\n",
    "            print(seq1_vec, seq2_vec)\n",
    "            print(seq1_vec.shape, seq2_vec.shape)\n",
    "            cos_sim = np.dot(seq1_vec,seq2_vec)/(np.linalg.norm(seq1_vec)*np.linalg.norm(seq2_vec))\n",
    "\n",
    "            '''NOT SETTLED ON WEIGHT OF COSINE SIMILARITY YET'''\n",
    "            cos_sim = (cos_sim*10)\n",
    "\n",
    "            # Add to matrix values via scoring method\n",
    "            diagonal += cos_sim\n",
    "            if gap is False:  # Apply gap_open penalty if there is no gap\n",
    "                horizontal += gopen\n",
    "                vertical += gopen\n",
    "            if gap is True:  # Apply gap_extension penalty if there is a gap\n",
    "                horizontal += gext\n",
    "                vertical += gext\n",
    "\n",
    "            # Update gap status\n",
    "            score = max(diagonal, horizontal, vertical)\n",
    "            if score == horizontal:\n",
    "                gap = True\n",
    "            if score == vertical:\n",
    "                gap = True\n",
    "            if score == diagonal:\n",
    "                gap = False\n",
    "\n",
    "            # Assign value to traceback matrix\n",
    "            if score == diagonal:\n",
    "                trace_m[i+1][j+1] = 0\n",
    "            if score == horizontal:\n",
    "                trace_m[i+1][j+1] = -1\n",
    "            if score == vertical:\n",
    "                trace_m[i+1][j+1] = 1\n",
    "\n",
    "            # Assign value to scoring matrix\n",
    "            score_m[i+1][j+1] = score\n",
    "\n",
    "    return trace_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_align(seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two sequences after gaps have been introduced and writes them to a file\n",
    "    in no particular format (yet).\n",
    "\n",
    "    :param seq1: first aligned sequence\n",
    "    :param seq2: second aligned sequence\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Add space every 10 characters\n",
    "    seq1 = [seq1[i:i+10] for i in range(0, len(seq1), 10)]\n",
    "    seq1 = ' '.join(seq1)\n",
    "    seq2 = [seq2[i:i+10] for i in range(0, len(seq2), 10)]\n",
    "    seq2 = ' '.join(seq2)\n",
    "\n",
    "    # Split sequences every 50 characters\n",
    "    seq1_split = [seq1[i:i+55] for i in range(0, len(seq1), 55)]\n",
    "    seq2_split = [seq2[i:i+55] for i in range(0, len(seq2), 55)]\n",
    "\n",
    "    # Find max length sequence and write to file based on its length\n",
    "    name1 = 'seque1'\n",
    "    name2 = 'seque2'\n",
    "    with open('PEbA_alignment.txt', 'w', encoding='utf8') as file:\n",
    "        file.write('PileUp\\n\\n\\n')\n",
    "        file.write(f'   MSF:  {len(seq1)}  Type:  P\\n\\n')\n",
    "        file.write(f'Name: {name1} oo  Len:  {len(seq1)}\\n')\n",
    "        file.write(f'Name: {name2} oo  Len:  {len(seq2)}\\n\\n//\\n\\n\\n\\n')\n",
    "        for i in range(len(seq1_split)):\n",
    "            file.write(f'{name1}      {seq1_split[i]}\\n')\n",
    "            file.write(f'{name2}      {seq2_split[i]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceback(trace_m, seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts a scoring and a traceback matrix and two sequences and returns global\n",
    "    alignment between the two sequences\n",
    "\n",
    "    :param trace_m: traceback matrix\n",
    "    :param seq1: first sequence\n",
    "    :param seq2: second sequence\n",
    "    return: highest scoring local alignment of the two sequences\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Reverse strings and convert to lists so gaps can be inserted\n",
    "    rev_seq1 = list(seq1[::-1])\n",
    "    rev_seq2 = list(seq2[::-1])\n",
    "\n",
    "    # Move through matrix starting at bottom right\n",
    "    rows, cols = trace_m.shape\n",
    "    index = [rows-1, cols-1]\n",
    "    count = 0\n",
    "    while index != [0, 0]:\n",
    "        val = trace_m[index[0], index[1]]\n",
    "        if val == 1:  # If cell is equal to 1, insert a gap into the second sequence\n",
    "            index[0] = max(index[0] - 1, 0)  # Taking max of new index and 0 so index never below 0\n",
    "            rev_seq2.insert(count, '.')\n",
    "        if val == -1:  # If cell is equal to -1, insert a gap into the first sequence\n",
    "            index[1] = max(index[1] - 1, 0)\n",
    "            rev_seq1.insert(count, '.')\n",
    "        if val == 0:  # If cell is equal to 0, there is no gap\n",
    "            index[0] = max(index[0] - 1, 0)\n",
    "            index[1] = max(index[1] - 1, 0)\n",
    "        count += 1\n",
    "\n",
    "    # Join lists and reverse strings again\n",
    "    seq1 = ''.join(rev_seq1)\n",
    "    seq2 = ''.join(rev_seq2)\n",
    "    seq1 = seq1[::-1]\n",
    "    seq2 = seq2[::-1]\n",
    "\n",
    "    # Introduce gaps at end of either sequence based off length of other sequence\n",
    "    seq1 = seq1+\".\"*max(0, len(seq2)-len(seq1))\n",
    "    seq2 = seq2+\".\"*max(0, len(seq1)-len(seq2))\n",
    "    write_align(seq1, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKSVK\n",
      "KVKKSVKAIYKVKKKSVKAIY\n"
     ]
    }
   ],
   "source": [
    "# Parse fasta files\n",
    "seq1 = parse_fasta('test1.fa')\n",
    "seq2 = parse_fasta('test2.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKSVK\n",
      "['K K S V K']\n",
      "[array([[-0.09522031, -0.14406678, -0.08793188, ...,  0.11808877,\n",
      "        -0.2478591 ,  0.00921371],\n",
      "       [ 0.21217607,  0.01319035, -0.05396955, ...,  0.4307134 ,\n",
      "        -0.18787546,  0.0230017 ],\n",
      "       [ 0.30360746, -0.11321975, -0.18153802, ...,  0.02647187,\n",
      "         0.12702607, -0.02317699],\n",
      "       [ 0.14964975, -0.08891524, -0.12834471, ...,  0.02811862,\n",
      "         0.0555214 ,  0.17308147],\n",
      "       [ 0.00992806, -0.01431647,  0.22271892, ...,  0.10562176,\n",
      "        -0.01820003, -0.22538298]], dtype=float32)]\n",
      "KVKKSVKAIYKVKKKSVKAIY\n",
      "['K V K K S V K A I Y K V K K K S V K A I Y']\n",
      "[array([[-8.48566666e-02,  2.06543244e-02, -1.97138995e-01, ...,\n",
      "         5.74776828e-01,  9.62849483e-02,  2.74258345e-01],\n",
      "       [ 9.24015641e-02,  2.07972765e-01, -2.83893049e-01, ...,\n",
      "         1.81381553e-01,  1.26026258e-01, -9.12059322e-02],\n",
      "       [-3.66858184e-01,  4.48539332e-02, -6.46745265e-02, ...,\n",
      "         1.05602242e-01,  2.25269645e-01, -2.63850943e-05],\n",
      "       ...,\n",
      "       [ 1.74862564e-01,  2.31018081e-01,  1.06075741e-01, ...,\n",
      "        -1.26285806e-01, -1.36517853e-01, -2.38955691e-01],\n",
      "       [ 4.00890708e-01,  1.05048925e-01, -3.79550755e-02, ...,\n",
      "         3.06800246e-01, -2.23269835e-01,  3.39180380e-01],\n",
      "       [ 6.11428022e-02,  1.43027082e-01,  2.49397114e-01, ...,\n",
      "        -3.11344713e-02, -5.99886440e-02, -1.27617717e-01]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize sequences\n",
    "vecs1 = embed_seq(seq1, tokenizer, model)\n",
    "vecs2 = embed_seq(seq2, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(vecs1[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08304115 -0.21574347 -0.02853072 ... -0.0790941  -0.49132112\n",
      "  -0.23327847]] [[ 0.08304115 -0.21574347 -0.02853072 ... -0.0790941  -0.49132112\n",
      "  -0.23327847]]\n",
      "(1, 1024) (1, 1024)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1024) and (1,1024) not aligned: 1024 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Call global_align() to get traceback matrix\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trace_m \u001b[39m=\u001b[39m global_align(seq1, seq2, vecs1, vecs2, \u001b[39m-\u001b[39;49m\u001b[39m3\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m, in \u001b[0;36mglobal_align\u001b[0;34m(seq1, seq2, vecs1, vecs2, gopen, gext)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(seq1_vec, seq2_vec)\n\u001b[1;32m     44\u001b[0m \u001b[39mprint\u001b[39m(seq1_vec\u001b[39m.\u001b[39mshape, seq2_vec\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 45\u001b[0m cos_sim \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(seq1_vec,seq2_vec)\u001b[39m/\u001b[39m(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(seq1_vec)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(seq2_vec))\n\u001b[1;32m     47\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''NOT SETTLED ON WEIGHT OF COSINE SIMILARITY YET'''\u001b[39;00m\n\u001b[1;32m     48\u001b[0m cos_sim \u001b[39m=\u001b[39m (cos_sim\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,1024) and (1,1024) not aligned: 1024 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Call global_align() to get traceback matrix\n",
    "trace_m = global_align(seq1, seq2, vecs1, vecs2, -3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call traceback() to get global alignment between seq1 and seq2\n",
    "traceback(trace_m, seq1, seq2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VectorizeAlignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14f7ba466a109ec4ba883675f85844c1da756d45ca64a51b0dffe815703534b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
