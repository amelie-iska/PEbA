{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5EncoderModel: ['decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.final_layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.embed_tokens.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import re\n",
    "\n",
    "# Load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "\n",
    "# Load into GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize protein sequences (taken from two PH domain proteins)\n",
    "seq1 = 'AGFISVISKKQGEYLEDEWY'\n",
    "seq2 = 'QVLDKFGS'\n",
    "#seq1 = 'MRSSASRLSSFSSRDSLWNRMPDQISVSEFIAETTEDYNSPTTSSFTTRLHNCRNTVTLLEEALDQDRTALQKVKKSVKAIYNSGQDHVQNEENYAQVLDKFGSNFLSRDNPDLGTAFVKFSTLTKELSTLLKNLLQGLSHNVIFTLDSLLKGDLKGVKGDLKKPFDKAWKDYETKFTKIEKEKREHAKQHGMIRTEITGAEIAEEMEKERRLFQLQMCEYLIKVNEIKTKKGVDLLQNLIKYYHAQCNFFQDGLKTADKLKQYIEKLAADLYNIKQTQDEEKKQLTALRDLIKSSLQLDQKEDSQSRQGGYSMHQLQGN'\n",
    "#seq2 = 'NTVTLLEEALDQDRTALQKVKKSVKAIYNSGQDHVQNEENYAQVLDKFGSNFLSRDNPDLGTAF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_align(seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two sequences after gaps have been introduced and writes them to a file\n",
    "    in no particular format (yet).\n",
    "\n",
    "    :param seq1: first aligned sequence\n",
    "    :param seq2: second aligned sequence\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    with open('alignment.txt', 'w', encoding='utf8') as file:\n",
    "        file.write('Smith-Waterman Pairwise Sequence Alignment\\n\\n\\n')\n",
    "        file.write(f'Sequence 1   {seq1}\\n')\n",
    "        file.write(f'Sequence 2    {seq2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_seqs(seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two protein sequences and returns a two lists of vectors, one list for\n",
    "    each protein where each vector represents one amino acid. Vectorization performed by the Rostlab\n",
    "    ProtT5-XL_UniRef50 model.\n",
    "\n",
    "    :param seq1: first protein sequence\n",
    "    :param seq2: second protein sequence\n",
    "    return: lists of vectorized amino acids\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Tokenize, encode, and load sequences\n",
    "    sequences = [' '.join([*seq1]), ' '.join([*seq2])]  # Add spaces between each amino acid\n",
    "    sequences = [re.sub(r\"[UZOB]\", \"X\", seq) for seq in sequences]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding=True)\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    # Extract sequence features\n",
    "    with torch.no_grad():\n",
    "        embedding = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "    embedding = embedding.last_hidden_state.cpu().numpy()\n",
    "\n",
    "    # Remove padding and special tokens\n",
    "    features = [] \n",
    "    for seq_num in range(len(embedding)):\n",
    "        seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "        seq_emd = embedding[seq_num][:seq_len-1]\n",
    "        features.append(seq_emd)\n",
    "\n",
    "    # Return lists of vectors\n",
    "    seq1 = features[0]\n",
    "    seq2 = features[1]\n",
    "    return seq1, seq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SW_align(seq1, seq2, vecs1, vecs2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts two sequences, creates a matrix corresponding to their lengths, and  \n",
    "    calculates the score of the alignments for each index. A second matrix is scored so that the\n",
    "    best alignment can be tracebacked.\n",
    "\n",
    "    :param seq1: first sequence\n",
    "    :param seq2: second sequence\n",
    "    :param vecs1: list of vectorized amino acids for first sequence\n",
    "    :param vecs2: list of vectorized amino acids for second sequence\n",
    "    return: scoring and traceback matrices of optimal scores for the SW-alignment of sequences\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # NCBI default gap costs\n",
    "    gap_open = -11\n",
    "    gap_ext = -1\n",
    "    gap = False\n",
    "\n",
    "    # Protein alphabet\n",
    "    chars = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    # Initialize scoring and traceback matrix based on sequence lengths\n",
    "    row_length = len(seq1)+1\n",
    "    col_length = len(seq2)+1\n",
    "    score_m = np.full((row_length, col_length), 0)\n",
    "    trace_m = np.full((row_length, col_length), 0)\n",
    "\n",
    "    # Score matrix by moving through each index\n",
    "    for i in range(len(seq1)):\n",
    "        seq1_char = seq1[i]  # Character in 1st sequence\n",
    "        seq1_vec = vecs1[i]  # Corresponding amino acid vector\n",
    "        for j in range(len(seq2)):\n",
    "            seq2_char = seq2[j]\n",
    "            \n",
    "            # Preceding scoring matrix values\n",
    "            diagonal = score_m[i][j]\n",
    "            horizontal = score_m[i+1][j]\n",
    "            vertical = score_m[i][j+1]\n",
    "\n",
    "            # Score residues based off cosine similarity between vectors\n",
    "            seq2_vec = vecs2[j]  # Corresponding amino acid vector\n",
    "            cos_sim = np.dot(seq1_vec,seq2_vec)/(np.linalg.norm(seq1_vec)*np.linalg.norm(seq2_vec))\n",
    "\n",
    "            '''TRYING OUT DIFFERENT COSINE SIMILARITY VALUE MODIFIERS'''\n",
    "            cos_sim = (cos_sim*10)\n",
    "\n",
    "            # Add to matrix values via scoring method\n",
    "            diagonal += cos_sim\n",
    "            if gap is False:  # Apply gap_open penalty if there is no gap\n",
    "                horizontal += gap_open\n",
    "                vertical += gap_open\n",
    "            if gap is True:  # Apply gap_extension penalty if there is a gap\n",
    "                horizontal += gap_ext\n",
    "                vertical += gap_ext\n",
    "\n",
    "            # Update gap status\n",
    "            score = max(diagonal, horizontal, vertical)\n",
    "            if score == horizontal: gap = True\n",
    "            if score == vertical: gap = True\n",
    "            if score == diagonal: gap = False\n",
    "\n",
    "            # Assign value to traceback matrix\n",
    "            if score == diagonal:\n",
    "                trace_m[i+1][j+1] = 0\n",
    "            if score == horizontal:\n",
    "                trace_m[i+1][j+1] = -1\n",
    "            if score == vertical:\n",
    "                trace_m[i+1][j+1] = -1\n",
    "\n",
    "            # Assign max value to scoring matrix\n",
    "            score_m[i+1][j+1] = max(score, 0)\n",
    "\n",
    "    return score_m, trace_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call vec_seqs() to get list of vectorized amino acids\n",
    "seq1_vecs, seq2_vecs = vec_seqs(seq1, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call SW_align()\n",
    "score_m, trace_m = SW_align(seq1, seq2, seq1_vecs, seq2_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  5,  3,  2,  1,  1,  1,  0,  0],\n",
       "       [ 0,  3,  8,  4,  3,  2,  1,  2,  0],\n",
       "       [ 0,  2,  6, 11,  5,  4,  5,  1,  2],\n",
       "       [ 0,  2,  4, 10, 12,  6,  5,  6,  1],\n",
       "       [ 0,  3,  3,  6, 12, 13,  7,  6,  7],\n",
       "       [ 0,  2,  6,  5,  7, 13, 15,  7,  6],\n",
       "       [ 0,  1,  4,  9,  6,  8, 15, 16,  7],\n",
       "       [ 0,  1,  1,  5, 11,  7,  9, 16, 17],\n",
       "       [ 0,  2,  1,  1,  7, 15,  8, 10, 17],\n",
       "       [ 0,  1,  2,  1,  2, 11, 16,  9, 11],\n",
       "       [ 0,  2,  2,  2,  1,  3, 12, 17, 10],\n",
       "       [ 0,  0,  3,  4,  3,  1,  4, 14, 17],\n",
       "       [ 0,  1,  0,  4,  7,  6,  1,  5, 15],\n",
       "       [ 0,  1,  3,  1,  5,  8,  8,  1,  5],\n",
       "       [ 0,  1,  3,  7,  2,  6, 11,  9,  1],\n",
       "       [ 0,  1,  2,  5,  9,  4,  7, 13, 10],\n",
       "       [ 0,  0,  1,  3,  9, 11,  6,  9, 14],\n",
       "       [ 0,  0,  0,  2,  5, 12, 13,  9, 11],\n",
       "       [ 0,  0,  1,  1,  3,  7, 15, 17, 11],\n",
       "       [ 0,  0,  0,  1,  1,  4,  9, 18, 23]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceback(score_m, trace_m, seq1, seq2):\n",
    "    \"\"\"=============================================================================================\n",
    "    This function accepts a scoring and a traceback matrix and two sequences and returns the highest \n",
    "    scoring local alignment between the two sequences\n",
    "\n",
    "    :param score_m: scoring matrix\n",
    "    :param trace_m: traceback matrix\n",
    "    :param seq1: first sequence\n",
    "    :param seq2: second sequence\n",
    "    return: highest scoring local alignment of the two sequences\n",
    "    =============================================================================================\"\"\"\n",
    "\n",
    "    # Find index of highest score in scoring matrix, start traceback at this matrix\n",
    "    high_score_ind = np.unravel_index(np.argmax(score_m, axis=None), score_m.shape)\n",
    "\n",
    "    # Reverse strings and convert to lists so gaps can be inserted\n",
    "    rev_seq1 = list(seq1[::-1])\n",
    "    rev_seq2 = list(seq2[::-1])\n",
    "\n",
    "    # Move through matrix starting at bottom right\n",
    "    index = [high_score_ind[0], high_score_ind[1]]\n",
    "    count = 0\n",
    "    while (index[0] and index[1]) != 0:\n",
    "        val = trace_m[index[0], index[1]]\n",
    "\n",
    "        if val == 1:  # If cell is equal to 1, insert a gap into the second sequence\n",
    "            index[0] = index[0] - 1\n",
    "            rev_seq2.insert(count, '_')\n",
    "        if val == -1:  # If cell is equal to -1, insert a gap into the first sequence\n",
    "            index[1] = index[1] - 1\n",
    "            rev_seq1.insert(count, '_')\n",
    "        if val == 0:  # If cell is equal to 0, there is no gap\n",
    "            index[0] = index[0] - 1\n",
    "            index[1] = index[1] - 1\n",
    "        count += 1\n",
    "\n",
    "    # Join lists and reverse strings again\n",
    "    seq1 = ''.join(rev_seq1)\n",
    "    seq2 = ''.join(rev_seq2)\n",
    "\n",
    "    # Need to store alignment data somehow, but until then we are printing spaces\n",
    "    space = ' '\n",
    "    print(index[1]*space+seq1[::-1])\n",
    "    print(index[0]*space+seq2[::-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGFISVISKKQGEYLEDEWY\n",
      "            QVLDKFGS\n"
     ]
    }
   ],
   "source": [
    "traceback(score_m, trace_m, seq1, seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VectorizeAlignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14f7ba466a109ec4ba883675f85844c1da756d45ca64a51b0dffe815703534b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
